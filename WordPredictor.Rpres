WordPredictor
========================================================
author: Gary Stocks
font-import: http://fonts.googleapis.com/css?family=Risque
font-family: 'Risque'
date: 14 November 2018
transition: rotate
autosize: true

Overview
========================================================
The objective of this model is to predict the next word from a sequence of words typed by a user. This can improve the usability of software applications which involve a lot of typing or smaller-sized interfaces where typing is difficult, such as those found on mobile devices.

A set of text documents from twitter, news feeds and blogs was used to train the model. This data was provided by SwiftKey. A 6-gram probabilistic language model was built using *Stupid Backoff* to rank next-word candidates.

The model achieved a 23.03% top-3 precision using an independent benchmark.


Next Word Prediction Algorithm
========================================================
The training data was used to build a table of n-grams from 1 to 6, with the frequency count of each n-gram from the observed training data.

The model takes the last 5 words entered as the n-gram prefix and finds n-grams which complete this prefix. Then the first word of the prefix is removed and n-grams which complete this reduced prefix are found. This process is repeated until arriving at 1-grams (unigrams). If fewer than 5 words was entered, the model uses all words entered as the first prefix. A score is calculated for the last word of each n-gram found, from the observed counts of each n-gram and the n-gram prefix. For each lower-order n-gram (5, 4, etc.), the score is weighted down by a factor of 0.4. The words with the highest scores are predicted as most likely.


Prediction Algorithm Performance
========================================================
To test the performance of the model, a [script](https://github.com/hfoffani/dsci-benchmark) was used which imports text from randomly selected tweets and blogs, and then evaluates the performance based on the top 3 predictions for each phrase. The results obtained were:
* Overall top-3 score: 18.91%
* Overall top-1 precision: 14.18%
* Overall top-3 precision: 23.03%
* Average runtime: 144.32 msec

The 23% top-3 precision means the model returns a correct prediction among its top 3 predictions in just over 1 out of every 5 predictions.


How to Use the Shiny App
========================================================
To use the app, navigate [here](add Shiny app URL) and enter a phrase in the text input box. Click *submit*. The app will return the top 3 most likely words to follow the phrase entered, in order from most likely to least likely. The top 10 most likely words will be plotted with their scores.

```{r, echo=FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
library(imager)
myimg <- load.image("D:/Users/gary.stocks/Desktop/Coursera/Course 10 Project/capstone/shinyapp.png")
plot(myimg)
```

