---
title: "Exploratory Text Mining"
author: "Gary Stocks"
date: "10 September 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)      # data manipulation & plotting
library(stringr)        # text cleaning and regular expressions
library(tidytext)       # provides additional text mining functions

```

### Overview
The objective of the capstone project is to develop a model to predict the next word a user will enter after having typed a few words. This can be used to make a mobile app more user friendly by limiting the typing a user needs to perform. The purpose of this exploratory analysis is to understand the nature of the text before building a model.

The dataset is made up of twitter, news and blog data. It was supplied by Swiftkey for the Johns Hopkins Data Science coursera capstone project, and was downloaded from [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). 
After loading the data, a subset is extracted due to the large dataset. This is explored to understand the characteristics of the data. Then pre-processing is used to prepare the data before analysing word and n-gram frequencies. Finally, the plan to build the model is explained.

The results ...

### Load Data
First, a function is defined to extract a sample of data from each of the US english (en_US) twitter, news and blogs files. Each of the 3 files are then sampled, and the result is stored in a local file. The table shows the size (in MB) and number of lines for each sample.

```{r load}

# Define a function to extract a random sample of 30% of each file and store in a file
sampleFile <- function(infile, outfile, header = TRUE) {
        set.seed(12345)
        ci <- file(infile, "r")
        co <- file(outfile, "w")
        if (header) {
                hdr <- readLines(ci, n = 1)
                writeLines(hdr, co)
        }
        recnum = 0
        numout = 0
        while (TRUE) {
                inrec <- readLines(ci, n = 1)
                if (length(inrec) == 0) { # end of file?
                        close(co)
                        close(ci)
                        return(numout)
                }
                recnum <- recnum + 1
                if (rbinom(1, 1, prob = .3) == 1) {
                        numout <- numout + 1
                        writeLines(inrec, co)
                }
        }
}

# Extract Twitter sample
t <- "D:/Users/gary.stocks/Desktop/Coursera/Course 10 Project/capstone/data/en_US/en_US.twitter.txt"
sampleFile(t, "twitter.txt", header = FALSE)

con <- file("D:/Users/gary.stocks/Desktop/Coursera/Course 10 Project/capstone/twitter.txt", "r")
twitter <- readLines(con) 
close(con) 

# Extract news sample
n <- "D:/Users/gary.stocks/Desktop/Coursera/Course 10 Project/capstone/data/en_US/en_US.news.txt"
sampleFile(n, "news.txt", header = FALSE)

con <- file("D:/Users/gary.stocks/Desktop/Coursera/Course 10 Project/capstone/news.txt", "r")
news <- readLines(con) 
close(con) 

# Extract blogs sample
b <- "D:/Users/gary.stocks/Desktop/Coursera/Course 10 Project/capstone/data/en_US/en_US.blogs.txt"
sampleFile(b, "blogs.txt", header = FALSE)

con <- file("D:/Users/gary.stocks/Desktop/Coursera/Course 10 Project/capstone/blogs.txt", "r")
blogs <- readLines(con) 
close(con) 

# Determine the size and number of lines in each sample file
samples <- data.frame(size_MB = as.numeric(), lines = as.integer())
samples[nrow(samples) + 1, ] = list(round(file.size("twitter.txt") / 1000000, 2), length(twitter))
samples[nrow(samples) + 1, ] = list(round(file.size("news.txt") / 1000000, 2), length(news))
samples[nrow(samples) + 1, ] = list(round(file.size("blogs.txt") / 1000000, 2), length(blogs))
row.names(samples) <- c("twitter", "news", "blogs")

samples

```

### Clean Text
The text is cleaned by removing retweets, special characters used in tweets, numbers and profanities. Then the most frequent words (excluding common 'stopwords') are visualised to check whether the cleaning has worked and to get an idea of what the texts are about. A simple correlation test is done to determine whether the word frequencies are similar across the twitter, news and blog sources.

```{r clean}

# Clean text of special characters and numbers
twitter <- gsub("&amp;|&lt;|&gt;", "", twitter, ignore.case = TRUE) # special characters
twitter <- gsub("[0-9]+", "", twitter) # numbers
twitter <- gsub("ðÿ", "", twitter, fixed = TRUE) # special characters
twitter <- gsub("â", "", twitter, fixed = TRUE) # special characters
twitter <- gsub("[^a-zA-Z]", " ", twitter)

news <- gsub("&amp;|&lt;|&gt;", "", news, ignore.case = TRUE) # special characters
news <- gsub("[0-9]+", "", news) # numbers
news <- gsub("ðÿ", "", news, fixed = TRUE) # special characters
news <- gsub("â", "", news, fixed = TRUE) # special characters
news <- gsub("[^a-zA-Z]", " ", news)

blogs <- gsub("&amp;|&lt;|&gt;", "", blogs, ignore.case = TRUE) # special characters
blogs <- gsub("[0-9]+", "", blogs) # numbers
blogs <- gsub("ðÿ", "", blogs, fixed = TRUE) # special characters
blogs <- gsub("â", "", blogs, fixed = TRUE) # special characters
blogs <- gsub("[^a-zA-Z]", " ", blogs)

# Create a tibble with a row for every word in every row of all 3 samples
titles <- c("Twitter", "News", "Blogs")
samples <- list(twitter, news, blogs)
series <- tibble()

for(i in seq_along(titles)) {
        
        clean <- tibble(row = seq_along(samples[[i]]),
                        text = samples[[i]]) %>%
                filter(!str_detect(text, "^RT")) %>%     # Remove retweets
                unnest_tokens(word, text) %>%
                mutate(source = titles[[i]]) %>%
                select(source, everything())
        
        series <- rbind(series, clean)
}

# Download a list of profanities
fileURL <- "https://community.jivesoftware.com/servlet/JiveServlet/download/1907-1-3237/profanity-list.zip"
temp <- tempfile()

if(!file.exists("profanity-list.csv")) {
        download.file(fileURL,temp, mode="wb")
        unzip(temp, "profanity-list.csv")
}

profanity <- read.csv("profanity-list.csv", header=FALSE)
profanity <- as.data.frame(profanity, stringsAsFactors = FALSE)
colnames(profanity) <- c("word")

# Remove profanities
series <- anti_join(series, profanity)

# Count word frequency
frequency <- series %>%
        count(word, sort = TRUE)

# Count the number of unique words
unigrams <- length(frequency$word)

# Find the subset of unique words in a frequency-sorted dictionary to cover 50% of all word instances in the language
topFeatures <- function(frequency, percent = .5) {
        count <- 0
        wordSum <- sum(frequency$n)
        wordDictionary <- data.frame(word = as.character(""), stringsAsFactors = FALSE)

        for(i in 1:length(frequency$n)) {
                count <- count + frequency$n[i] 
                wordDictionary$word[i] <- frequency$word[i]
                
                if(count / wordSum >= percent) {
                        return(wordDictionary)
                        } 
                
                wordDictionary[nrow(wordDictionary) + 1, ] = as.character("")
        }
}

# Calculate the number of words which cover 50% and 90% of all word instances
words50 <- length(topFeatures(frequency, percent = .5)[[1]])
words90 <- length(topFeatures(frequency, percent = .9)[[1]])

# Remove stop words to see what the common words are besides the stop words
series %>%
        anti_join(stop_words) %>%
        count(word, sort = TRUE)

# Find the top 20 most common words from each source
common <- series %>%
        anti_join(stop_words) %>%
        group_by(source) %>%
        count(word, sort = TRUE) %>%
        top_n(20)

# Visualise the top 20 most common words from each source
series %>%
        anti_join(stop_words) %>%
        group_by(source) %>%
        count(word, sort = TRUE) %>%
        top_n(20) %>%
        ungroup() %>%
        mutate(source = factor(source, levels = titles),
               text_order = nrow(.):1) %>%
        ggplot(aes(reorder(word, text_order), n, fill = source)) +
        geom_bar(stat = "identity") +
        facet_wrap(~ source, scales = "free_y") +
        labs(x = "NULL", y = "Frequency") +
        coord_flip() +
        theme(legend.position="none")

# Calculate percent of word use across all sources
source_pct <- series %>%
        anti_join(stop_words) %>%
        count(word) %>%
        transmute(word, all_words = n / sum(n))

# Calculate percent of word use within each source
frequency <- series %>%
        anti_join(stop_words) %>%
        count(source, word) %>%
        mutate(source_words = n / sum(n)) %>%
        left_join(source_pct) %>%
        arrange(desc(source_words)) %>%
        ungroup()

# Quantify how similar and different these sets of word frequencies are using a correlation test
frequency %>%
        group_by(source) %>%
        summarize(correlation = cor(source_words, all_words),
                  p_value = cor.test(source_words, all_words)$p.value)

```

There are `r unigrams` unigrams (individual words). Out of the total word count, 50% is covered by `r words50` words and 90% is covered by `r words90` words. The simple correlation test results in p-values of 0 for each correlation, which suggests a high degree of commonality in common words across all sources.

### Explore Bigrams and Trigrams
The frequency of bigrams and trigrams are extracted from the data to understand common 2-word and 3-word combinations.

```{r bigrams}

titles <- c("Twitter", "News", "Blogs")
samples <- list(twitter, news, blogs)
series <- tibble()

# Bigrams
for(i in seq_along(titles)) {
        
        clean <- tibble(row = seq_along(samples[[i]]),
                        text = samples[[i]]) %>%
                filter(!str_detect(text, "^RT")) %>%     # Remove retweets
                unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
                mutate(source = titles[i]) %>%
                select(source, everything())
        
        series <- rbind(series, clean)
}

# Count the number of unique bigrams
bigrams <- length(unique(series$bigram))

# Visualise the top 20 bigrams for each source
series %>%
        separate(bigram, c("word1", "word2"), sep = " ") %>%
        filter(!word1 %in% stop_words$word,
               !word2 %in% stop_words$word) %>%
        filter(!word1 %in% profanity$word,
               !word2 %in% profanity$word) %>%
        filter(!word1 %in% NA,
               !word2 %in% NA) %>%
        count(source, word1, word2, sort = TRUE) %>%
        unite("bigram", c(word1, word2), sep = " ") %>%
        group_by(source) %>%
        top_n(20) %>%
        ungroup() %>%
        mutate(source = factor(source) %>% forcats::fct_rev()) %>%
        ggplot(aes(bigram, n, source, n, fill = source)) +
        geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
        facet_wrap(~ source, ncol = 2, scales = "free") +
        coord_flip() 

```

There are `r bigrams` bigrams (2-word combinations).

```{r trigrams}

# Create trigrams
series <- tibble()

for(i in seq_along(titles)) {
        
        clean <- tibble(row = seq_along(samples[[i]]),
                        text = samples[[i]]) %>%
                filter(!str_detect(text, "^RT")) %>%     # Remove retweets
                unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
                mutate(source = titles[i]) %>%
                select(source, everything())
        
        series <- rbind(series, clean)
}

# Count the number of unique trigrams
trigrams <- length(unique(series$trigram))

# Visualise the top 20 tri-grams for each source
series %>%
        separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
        filter(!word1 %in% stop_words$word,
               !word2 %in% stop_words$word,
               !word3 %in% stop_words$word) %>%
        filter(!word1 %in% profanity$word,
               !word2 %in% profanity$word,
               !word3 %in% profanity$word) %>%
        filter(!word1 %in% NA,
               !word2 %in% NA,
               !word3 %in% NA) %>%
        count(source, word1, word2, word3, sort = TRUE) %>%
        unite("trigram", c(word1, word2, word3), sep = " ") %>%
        group_by(source) %>%
        top_n(20) %>%
        ungroup() %>%
        mutate(source = factor(source) %>% forcats::fct_rev()) %>%
        ggplot(aes(trigram, n, source, n, fill = source)) +
        geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
        facet_wrap(~ source, ncol = 2, scales = "free") +
        coord_flip()

```

There are `r trigrams` trigrams (3-word combinations). 

In summary, the sample contains:
- `r unigrams` unigrams
- `r bigrams` bigrams
- `r trigrams` trigrams

### Plan for Model Development
The basic plan to develop a predictive text model includes:
- Do additional text cleaning on 2-character unigrams which look like abbreviations and colloquial words (e.g. 'ii', 've', 'blah')
- Divide the text into training and testing sets
- Create a quanteda corpus with the training data
- Insert start-of-sentence and end-of-sentence markers before extracting n-grams
- Potentially remove singletons (n-grams with a frequency of 1) to improve performance
- Create data tables for each n-gram (unigrams, bigrams, trigrams and so on) with a frequency count for each n-gram
- Investigate smoothing and / or additional sources for out-of-bag words
- Use back off techniques to establish probabilities for otherwise unseen elements of an n-gram
- Decide on n in n-gram model
- Test and refine the model
_ Investigate ways of improving performance, such as writing the data table to a SQL table